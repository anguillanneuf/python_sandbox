{"cells":[{"cell_type":"markdown","source":["__2017-04-19:__ build property graph\n\n|quarter|records|\n|---|---|\n|17Q1|21569108 |\n\n__2017-04-25:__ analyze @midnight"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F; from pyspark.sql.types import *; from pyspark.sql import Window, Row\nimport graphframes as G\nimport datetime as dt; import numpy as np; import networkx as nx; import re; import pandas as pd"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["e = sqlContext.read.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1/edges')\nv = sqlContext.read.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1/vertices')\n\ng = G.GraphFrame(v, e)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(v.filter(F.col('program').rlike(\"MIDNIGHT\")))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Part I  \n1. isolate pids that watch either `CMDY_AT MIDNIGHT_O_PRIME TIME` or `CMDY_AT MIDNIGHT_R_NIGHT TIME`  \n2. create subgraph using these pids  \n3. check out their `WEEKDAY DAYTIME` edges  \n5. competitive programming in `WEEKDAY DAYTIME`"],"metadata":{}},{"cell_type":"code","source":["help(g.bfs)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# create expression for pids \n\npids_expr = '('\n\nfor p in pids:\n  pids_expr = pids_expr+(p[0])+','\n  \npids_expr = pids_expr[:-1] + ')'"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# step 1, 2\n\npids = (e.select('pid', F.col('src').rlike('^CMDY_AT MIDNIGHT_')).select('pid').dropDuplicates().collect())\n\ne2 = (e.filter('pid in '+pids_expr)) \n\ng2 = G.GraphFrame(v, e2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# step 3,5 weekday daytime competitors\n\nprint(g2.vertices.filter('daypart = \"WEEKDAY DAYTIME\"').orderBy('wt_min_tot', ascending = False).show(15, False))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["pr = g2.pageRank(0.15, 'CMDY_AT MIDNIGHT_R_NIGHT TIME', 10).cache()\nprint(pr.vertices.orderBy('pagerank', ascending=False).filter('daypart=\"WEEKDAY DAYTIME\"').show(15, False))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["pr2 = g2.pageRank(0.15, 'CMDY_AT MIDNIGHT_O_PRIME TIME', 10).cache()\nprint(pr2.vertices.orderBy('pagerank', ascending=False).filter('daypart=\"WEEKDAY DAYTIME\"').show(15, False))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Part II  \n1. Construct graph with `day_hour` as nodes  \n2. calculate pagerank by weekday and hour"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1_hour/dat')\nprint(df.show(10))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["pids = (df.select('pid', F.col('program').rlike('^AT MIDNIGHT'), F.col('net').rlike('^CMDY')).select('pid').dropDuplicates().collect())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["print(len(pids))\n\npids_expr = '('\n\nfor p in pids:\n  pids_expr = pids_expr+(p[0])+','\n  \npids_expr = pids_expr[:-1] + ')'"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# vertices for At Midnight viewers\n\nv = (df\n     .filter('pid in '+pids_expr)\n     .select(F.substring_index(df.id, '_', -2).alias('id'), 'daypart', 'day', 'hour', 'wt_min_tot')\n     .dropDuplicates()\n     .groupby(['id', 'daypart']).agg({\"wt_min_tot\": \"sum\"})\n     .orderBy('id')\n     .cache())\n\nprint(v.show(10, False))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# edges for At Midnight viewers\n\nHEAD = Row(\"id\", \"pid\", \"age\", \"gender\", \"net\", \"program\", \"date\", \"raw\", \"avg_wt_min\", \"ind\", \n           \"wt_min\", \"daypart\", \"program_id\", \"hour\", \"day\", \"avg_wt_min_tot\", \"wt_min_tot\", \"dur\", \"freq\", \"wt_min_shr\")\n\nhead = HEAD('HEAD_HEAD_HEAD_HEAD', 'HEAD', 1, 'HEAD', 'HEAD', 'HEAD', dt.date(1970,1,1), 1, 1, 'HEAD',\n           1, 'HEAD', 'HEAD', 'HEAD', 'HEAD', 1, 1, 1, 1, 1)\n\nw = Window.partitionBy('pid')\n\ndef f(x): return x\n\ne = (df\n      .filter('pid in '+pids_expr).withColumn('cnt', F.count('pid').over(w))\n      .orderBy(['pid', 'date'])\n      .rdd\n      .map(lambda x: (x[1], (x)))\n#       .reduceByKey(lambda a, b: (a if type(a)==list else [a])+(b if type(b)==list else [b]))\n      .groupByKey().mapValues(list)\n      .map(lambda x: (x[0], [head]+x[1]))\n      .map(lambda x: (x[0], zip(x[1][::1], x[1][1::1])))\n      .flatMapValues(f)\n      .map(lambda x: (x[0], x[1][0][0], x[1][1][0]) + tuple(x[1][1][i] for i in [2,3,6,10])))\n\nprint(e.take(3))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["schema = StructType([StructField(\"pid\", StringType(), nullable=True),\n                     StructField(\"src\", StringType(), True), \n                     StructField(\"dst\", StringType(), True),\n                     StructField(\"age\", ShortType(), True),\n                     StructField(\"gender\", StringType(), True),\n                     StructField(\"date\", DateType(), True),\n                     StructField(\"wt_min\", IntegerType(), True)])\n\n(spark\n .createDataFrame(e, schema=schema)\n .withColumn('src', F.substring_index('src', '_', -2))\n .withColumn('dst', F.substring_index('dst', '_', -2))\n .write.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1_hour/edges', mode = 'overwrite'))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["v.withColumnRenamed('sum(wt_min_tot)', 'wt_min_tot').write.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1_hour/vertices', mode = 'overwrite')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["v = spark.read.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1_hour/vertices')\ne = spark.read.parquet('/mnt/vmn.tianzi/graphframes/l3d_17q1_hour/edges')\nprint(v.show(2))\nprint(e.show(2))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# initialize an empty graph for 2016. \nG = nx.Graph()\n\n# prepare vertices.\nvl = [(i.id,i.asDict()) for i in v.rdd.collect()]\nprint(\"Vertex example: \\n{}\".format(vl[0]))\n\n# add vertices.\nG.add_nodes_from(vl)\n\n# prepare edges.\nel =  [(i.src, i.dst, i.asDict()) for i in e.rdd.collect()]\nprint(\"Edge example: \\n{}\".format(el[0]))\n\n# add edges.\nG.add_edges_from(el)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# pg = nx.pagerank(G, weight = 'wt_min_tot', max_iter = 20)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["temp = v.select('id', 'wt_min_tot').rdd.collect()\ntemp = [{t.id: t.wt_min_tot} for t in temp]\np_dict = dict()\nfor t in temp:\n  p_dict.update(t)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["G.remove_node('HEAD_HEAD')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["pg = nx.pagerank(G, personalization = p_dict, weight = 'wt_min', max_iter = 20)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["pg_pd = pd.DataFrame({'id': pg.keys(), 'pagerank': pg.values()})"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["pg_df = spark.createDataFrame(pg_pd)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(pg_df.select('id', F.split('id', '_')[0].alias('day'), F.split('id', '_')[1].alias('hour'), 'pagerank'))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["mypg = pg_df.select('id', F.split('id', '_')[0].alias('day'), F.split('id', '_')[1].alias('hour'), 'pagerank').toPandas()\n\n# Define the sorter\nsorter = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# Create the dictionary that defines the order for sorting\n# sorterIndex = dict(zip(sorter,range(len(sorter))))\n\n# mypg['day_Rank'] = mypg['day'].map(sorterIndex)\n\n# mypg.sort(['day_Rank', 'hour', 'day'], ascending = [True, True, True], inplace = True)\n\n# mypg.drop('day_Rank', 1, inplace = True)\n\nmypg = mypg.pivot(index='hour', columns='day', values='pagerank')\n\nmypg = mypg.reindex_axis(sorter, axis=1)\n\nmypg"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["import seaborn as sns; import matplotlib.pyplot as plt;"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["fig, ax = plt.subplots()\nax = sns.heatmap(mypg, fmt=\"d\", cmap=\"RdBu_r\", linewidths=.5)\nax.set_xlabel(''); ax.set_ylabel('')\n\nfor item in ax.get_yticklabels():\n    item.set_rotation(0)\n    item.set_fontsize(9)\n    \nfor item in ax.get_xticklabels():\n    item.set_fontsize(9)\n    \ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"05. midnight","notebookId":37658},"nbformat":4,"nbformat_minor":0}
